"""
OpenAlex æœŸåˆŠæ•°æ®æº

é€šè¿‡ OpenAlex API è·å–å­¦æœ¯æœŸåˆŠçš„æœ€æ–°è®ºæ–‡å…ƒæ•°æ®ã€‚
ç›¸æ¯” Crossrefï¼ŒOpenAlex æä¾›æ›´å®Œæ•´çš„æ‘˜è¦å’Œå…ƒæ•°æ®ã€‚
"""

import json
import logging
import re
import traceback
import requests
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Optional

from .base_source import BasePaperSource, PaperMetadata

logger = logging.getLogger(__name__)

# æœŸåˆŠåç§°åˆ° ISSN çš„æ˜ å°„ï¼ˆä¸ Crossref ä¿æŒä¸€è‡´ï¼‰
JOURNAL_ISSN_MAP = {
    # Physical Review ç³»åˆ—
    "prl": {
        "full_name": "Physical Review Letters",
        "issn": ["0031-9007", "1079-7114"],
        "display_name": "PRL"
    },
    "pra": {
        "full_name": "Physical Review A",
        "issn": ["2469-9926", "1050-2947"],
        "display_name": "PRA"
    },
    "prb": {
        "full_name": "Physical Review B",
        "issn": ["2469-9950", "1098-0121"],
        "display_name": "PRB"
    },
    "prc": {
        "full_name": "Physical Review C",
        "issn": ["2469-9985", "0556-2813"],
        "display_name": "PRC"
    },
    "prd": {
        "full_name": "Physical Review D",
        "issn": ["2470-0010", "1550-7998"],
        "display_name": "PRD"
    },
    "pre": {
        "full_name": "Physical Review E",
        "issn": ["2470-0045", "1539-3755"],
        "display_name": "PRE"
    },
    "prx": {
        "full_name": "Physical Review X",
        "issn": ["2160-3308"],
        "display_name": "PRX"
    },
    "prxq": {
        "full_name": "PRX Quantum",
        "issn": ["2691-3399"],
        "display_name": "PRX Quantum"
    },
    "rmp": {
        "full_name": "Reviews of Modern Physics",
        "issn": ["0034-6861", "1539-0756"],
        "display_name": "RMP"
    },
    # Nature ç³»åˆ—
    "nature": {
        "full_name": "Nature",
        "issn": ["0028-0836", "1476-4687"],
        "display_name": "Nature"
    },
    "nature_physics": {
        "full_name": "Nature Physics",
        "issn": ["1745-2473", "1745-2481"],
        "display_name": "Nat. Phys."
    },
    "nature_communications": {
        "full_name": "Nature Communications",
        "issn": ["2041-1723"],
        "display_name": "Nat. Commun."
    },
    # Science ç³»åˆ—
    "science": {
        "full_name": "Science",
        "issn": ["0036-8075", "1095-9203"],
        "display_name": "Science"
    },
    "science_advances": {
        "full_name": "Science Advances",
        "issn": ["2375-2548"],
        "display_name": "Sci. Adv."
    },
    # å…¶ä»–é‡è¦æœŸåˆŠ
    "npj_quantum_information": {
        "full_name": "npj Quantum Information",
        "issn": ["2056-6387"],
        "display_name": "npj QI"
    },
    "quantum": {
        "full_name": "Quantum",
        "issn": ["2521-327X"],
        "display_name": "Quantum"
    },
    "new_journal_of_physics": {
        "full_name": "New Journal of Physics",
        "issn": ["1367-2630"],
        "display_name": "NJP"
    },
}


class OpenAlexSource(BasePaperSource):
    """
    OpenAlex æœŸåˆŠæ•°æ®æºã€‚

    ç‰¹ç‚¹ï¼š
    - æ”¯æŒå¤šç§å­¦æœ¯æœŸåˆŠï¼ˆPRLã€PRAã€Nature ç­‰ï¼‰
    - é€šè¿‡ OpenAlex API è·å–å…ƒæ•°æ®
    - æä¾›å€’æ’ç´¢å¼•æ ¼å¼çš„æ‘˜è¦ï¼ˆè‡ªåŠ¨é‡å»ºä¸ºæ–‡æœ¬ï¼‰
    - ä¸æ”¯æŒ PDF ä¸‹è½½ï¼Œä»…è¿›è¡Œè¯„åˆ†åˆ†æ
    """

    API_BASE_URL = "https://api.openalex.org"

    def __init__(
        self,
        history_dir: Path,
        journals: List[str] = None,
        max_results: int = 100,
        email: str = None,
        api_key: str = None
    ):
        """
        åˆå§‹åŒ– OpenAlex æ•°æ®æºã€‚

        å‚æ•°:
            history_dir: å†å²è®°å½•å­˜å‚¨ç›®å½•
            journals: è¦æŠ“å–çš„æœŸåˆŠä»£ç åˆ—è¡¨ï¼Œå¦‚ ["prl", "pra"]
            max_results: æ¯ä¸ªæœŸåˆŠæœ€å¤šæŠ“å–çš„è®ºæ–‡æ•°
            email: ç”¨æˆ·é‚®ç®±ï¼ˆç”¨äºç¤¼è²Œæ± ï¼Œæé«˜é€Ÿç‡é™åˆ¶ï¼‰
            api_key: OpenAlex API Keyï¼ˆå¯é€‰ï¼Œ2026å¹´2æœˆåå¿…éœ€ï¼‰
        """
        super().__init__("openalex", history_dir)
        self.journals = journals or []
        self.max_results = max_results
        self.email = email
        self.api_key = api_key

        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": "ArxivDailyResearcher/2.0 (https://github.com/yzr278892/arxiv-daily-researcher; yzr278892@gmail.com)"
        })

    def __enter__(self):
        """æ”¯æŒä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """é€€å‡ºæ—¶å…³é—­Session"""
        self.close()

    def close(self):
        """å…³é—­ç½‘ç»œè¿æ¥"""
        if self.session:
            self.session.close()
            logger.debug("OpenAlex Sessionå·²å…³é—­")

    @property
    def display_name(self) -> str:
        return "OpenAlex"

    def can_download_pdf(self) -> bool:
        return False  # OpenAlex åªæä¾›å…ƒæ•°æ®

    def get_journal_info(self, journal_code: str) -> Optional[Dict]:
        """è·å–æœŸåˆŠä¿¡æ¯"""
        return JOURNAL_ISSN_MAP.get(journal_code.lower())

    def fetch_papers(
        self,
        days: int,
        journals: List[str] = None,
        **kwargs
    ) -> List[PaperMetadata]:
        """
        ä» OpenAlex æŠ“å–æŒ‡å®šæœŸåˆŠæœ€è¿‘ N å¤©çš„è®ºæ–‡ã€‚

        å‚æ•°:
            days: æœç´¢æœ€è¿‘ N å¤©çš„è®ºæ–‡
            journals: æœŸåˆŠä»£ç åˆ—è¡¨ï¼Œå¦‚ ["prl", "pra"]

        è¿”å›:
            List[PaperMetadata]: è®ºæ–‡å…ƒæ•°æ®åˆ—è¡¨
        """
        if journals:
            self.journals = journals

        if not self.journals:
            logger.warning("[OpenAlex] æœªæŒ‡å®šæœŸåˆŠï¼Œè·³è¿‡æŠ“å–")
            return []

        all_papers = []
        from_date = (datetime.now() - timedelta(days=days)).strftime("%Y-%m-%d")

        logger.info(f"[OpenAlex] å¼€å§‹æŠ“å–æœŸåˆŠè®ºæ–‡")
        logger.info(f"  ç›®æ ‡æœŸåˆŠ: {self.journals}")
        logger.info(f"  æ—¶é—´èŒƒå›´: æœ€è¿‘ {days} å¤©ï¼ˆä» {from_date}ï¼‰")

        for journal_code in self.journals:
            journal_info = self.get_journal_info(journal_code)
            if not journal_info:
                logger.warning(f"  æœªçŸ¥æœŸåˆŠä»£ç : {journal_code}ï¼Œè·³è¿‡")
                continue

            issn_list = journal_info["issn"]
            journal_name = journal_info["full_name"]
            display_name = journal_info["display_name"]

            logger.info(f"  æ­£åœ¨æŠ“å– {journal_name}...")

            try:
                papers = self._fetch_journal_papers(
                    issn_list=issn_list,
                    journal_code=journal_code,
                    journal_name=journal_name,
                    from_date=from_date
                )
                all_papers.extend(papers)
                logger.info(f"    {display_name}: å‘ç° {len(papers)} ç¯‡æ–°è®ºæ–‡")

            except Exception as e:
                logger.error(f"    {display_name} æŠ“å–å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()

        logger.info(f"[OpenAlex] æ€»è®¡å‘ç° {len(all_papers)} ç¯‡æ–°è®ºæ–‡")
        return all_papers

    def _fetch_from_arxiv(self, arxiv_id: str, journal_code: str, journal_name: str, doi: str) -> Optional[PaperMetadata]:
        """
        é€šè¿‡ arXiv ID ä» ArXiv è·å–è®ºæ–‡å…ƒæ•°æ®ã€‚

        å‚æ•°:
            arxiv_id: arXiv ID
            journal_code: æœŸåˆŠä»£ç 
            journal_name: æœŸåˆŠå…¨å
            doi: DOI

        è¿”å›:
            Optional[PaperMetadata]: è®ºæ–‡å…ƒæ•°æ®ï¼Œå¤±è´¥æ—¶è¿”å› None
        """
        try:
            import arxiv

            # ä½¿ç”¨ arXiv API è·å–è®ºæ–‡
            search = arxiv.Search(id_list=[arxiv_id])
            client = arxiv.Client(
                page_size=1,
                delay_seconds=3.0,
                num_retries=2
            )

            results = list(client.results(search))
            if not results:
                logger.warning(f"    âš ï¸  arXiv API æœªæ‰¾åˆ°è®ºæ–‡: {arxiv_id}")
                return None

            result = results[0]

            # è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼ï¼Œä¿ç•™æœŸåˆŠä¿¡æ¯
            metadata = PaperMetadata(
                paper_id=result.get_short_id(),
                title=result.title,
                authors=[author.name for author in result.authors],
                abstract=result.summary,  # arXiv æä¾›å®Œæ•´æ‘˜è¦
                published_date=result.published,
                url=result.entry_id,
                source=journal_code,  # ä¿ç•™æœŸåˆŠä»£ç 
                pdf_url=result.pdf_url,
                doi=doi,  # ä½¿ç”¨æœŸåˆŠçš„ DOI
                journal=journal_name,  # æ ‡æ³¨æœŸåˆŠåç§°
                arxiv_id=arxiv_id,
                arxiv_url=result.entry_id,
                categories=list(result.categories) if result.categories else []
            )

            logger.info(f"    âœ… [{result.title[:30]}...] ä½¿ç”¨ arXiv æºè·å–å®Œæ•´å…ƒæ•°æ® (arXiv:{arxiv_id})")
            return metadata

        except Exception as e:
            logger.warning(f"    âš ï¸  ä» arXiv è·å–è®ºæ–‡å¤±è´¥ ({arxiv_id}): {e}")
            return None

    def _fetch_journal_papers(
        self,
        issn_list: List[str],
        journal_code: str,
        journal_name: str,
        from_date: str
    ) -> List[PaperMetadata]:
        """
        æŠ“å–å•ä¸ªæœŸåˆŠçš„è®ºæ–‡ã€‚

        å‚æ•°:
            issn_list: æœŸåˆŠ ISSN åˆ—è¡¨
            journal_code: æœŸåˆŠä»£ç ï¼ˆç”¨äº source å­—æ®µï¼‰
            journal_name: æœŸåˆŠå…¨å
            from_date: èµ·å§‹æ—¥æœŸ (YYYY-MM-DD)

        è¿”å›:
            List[PaperMetadata]: è®ºæ–‡åˆ—è¡¨
        """
        papers = []

        # æ„å»º ISSN è¿‡æ»¤å™¨ï¼ˆæ”¯æŒå¤šä¸ªISSNï¼‰
        issn_filter = "|".join(issn_list)

        url = f"{self.API_BASE_URL}/works"

        # æ·»åŠ é‚®ç®±æˆ–API Keyåˆ°åŸºç¡€å‚æ•°
        base_params = {}
        if self.api_key:
            base_params["api_key"] = self.api_key
        elif self.email:
            base_params["mailto"] = self.email

        # å®ç°åˆ†é¡µé€»è¾‘ï¼Œæ”¯æŒè·å–è¶…è¿‡200æ¡çš„ç»“æœ
        page = 1
        per_page = min(200, self.max_results)  # OpenAlexå•é¡µæœ€å¤§200
        total_fetched = 0

        try:
            while total_fetched < self.max_results:
                params = {
                    "filter": f"primary_location.source.issn:{issn_filter},from_publication_date:{from_date}",
                    "per_page": per_page,
                    "page": page,
                    "sort": "publication_date:desc",
                    "select": "id,doi,title,authorships,abstract_inverted_index,publication_date,primary_location,open_access,locations,best_oa_location,ids"
                }
                params.update(base_params)

                logger.debug(f"  æ­£åœ¨è·å–ç¬¬ {page} é¡µ...")
                response = self.session.get(url, params=params, timeout=30)
                response.raise_for_status()
                data = response.json()

                results = data.get("results", [])
                if not results:
                    logger.debug(f"  ç¬¬ {page} é¡µæ— æ›´å¤šç»“æœï¼Œåœæ­¢åˆ†é¡µ")
                    break

                for item in results:
                    doi = item.get("doi")
                if not doi:
                    # ä½¿ç”¨ OpenAlex ID ä½œä¸ºåå¤‡
                    openalex_id = item.get("id", "").replace("https://openalex.org/", "")
                    if not openalex_id:
                        continue
                    doi = f"openalex:{openalex_id}"

                # å»é‡æ£€æŸ¥
                if self.is_processed(doi):
                    continue

                # æå–æ ‡é¢˜
                title = item.get("title", "Untitled")
                if not title or title == "Untitled":
                    continue

                    # æ¸…ç†æ ‡é¢˜ï¼ˆç§»é™¤å¯èƒ½çš„HTMLæ ‡ç­¾ï¼‰
                    title = re.sub(r'<[^>]+>', '', title)
                    title = re.sub(r'\s+', ' ', title).strip()

                    # æå–ä½œè€…
                    authors = []
                    authorships = item.get("authorships", [])
                    for authorship in authorships[:20]:  # æœ€å¤š20ä¸ªä½œè€…
                        author = authorship.get("author", {})
                        display_name = author.get("display_name")
                        if display_name:
                            authors.append(display_name)

                    # æå–å¹¶é‡å»ºæ‘˜è¦
                    abstract = ""
                    inverted_index = item.get("abstract_inverted_index")
                    if inverted_index:
                        abstract = self._rebuild_abstract(inverted_index)
                        logger.debug(f"    âœ… [{title[:30]}...] æˆåŠŸè·å–æ‘˜è¦")
                    else:
                        logger.warning(f"    âš ï¸  [{title[:30]}...] OpenAlex æœªæä¾›æ‘˜è¦æ•°æ® (å¯èƒ½å› æœŸåˆŠç‰ˆæƒé™åˆ¶)")

                    # æå–å‘å¸ƒæ—¥æœŸ
                    pub_date_str = item.get("publication_date")
                    published_date = self._parse_date(pub_date_str)

                    # æå– URL
                    landing_page_url = doi if doi.startswith("http") else f"https://doi.org/{doi.replace('openalex:', '')}"
                    primary_location = item.get("primary_location", {})
                    if primary_location and primary_location.get("landing_page_url"):
                        landing_page_url = primary_location["landing_page_url"]

                    # æå– PDF URLï¼ˆå¦‚æœå¼€æ”¾è·å–ï¼‰
                    pdf_url = None
                    open_access = item.get("open_access", {})
                    if open_access.get("is_oa") and open_access.get("oa_url"):
                        pdf_url = open_access["oa_url"]
                        logger.debug(f"    âœ… [{title[:30]}...] æ‰¾åˆ°å¼€æ”¾è·å– PDF")

                    # ä» locations æå– arXiv ä¿¡æ¯ï¼ˆä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æé«˜å¥å£®æ€§ï¼‰
                    arxiv_id = None
                    arxiv_url = None
                    locations = item.get("locations", [])
                    for loc in locations:
                        source_info = loc.get("source", {})
                        if source_info:
                            source_name = source_info.get("display_name", "")
                            # æ£€æŸ¥æ˜¯å¦æ˜¯ arXiv æ¥æº
                            if "arxiv" in source_name.lower():
                                loc_url = loc.get("landing_page_url", "")
                                if loc_url and "arxiv.org" in loc_url:
                                    arxiv_url = loc_url
                                    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå– arXiv IDï¼Œæ›´å¥å£®
                                    try:
                                        match = re.search(r'arxiv\.org/(?:abs|pdf)/(\d{4}\.\d{4,5})', loc_url)
                                        if match:
                                            arxiv_id = match.group(1)
                                    except Exception as e:
                                        logger.debug(f"arXiv IDæå–å¤±è´¥: {e}")
                                    break

                    # ğŸ¯ ä¼˜å…ˆç­–ç•¥ï¼šå¦‚æœæ‰¾åˆ° arXiv ç‰ˆæœ¬ï¼Œä½¿ç”¨ ArXiv æºè·å–å®Œæ•´å…ƒæ•°æ®
                    if arxiv_id:
                        logger.info(f"    ğŸ”„ [{title[:30]}...] æ£€æµ‹åˆ° arXiv ç‰ˆæœ¬: {arxiv_id}ï¼Œè½¬è€Œä½¿ç”¨ ArXiv æºè·å–å®Œæ•´å…ƒæ•°æ®")
                        arxiv_metadata = self._fetch_from_arxiv(arxiv_id, journal_code, journal_name, doi)
                        if arxiv_metadata:
                            papers.append(arxiv_metadata)
                            total_fetched += 1
                            if total_fetched >= self.max_results:
                                break
                            continue  # è·³è¿‡ OpenAlex çš„å…ƒæ•°æ®æå–ï¼Œç›´æ¥å¤„ç†ä¸‹ä¸€ç¯‡è®ºæ–‡
                        else:
                            logger.warning(f"    âš ï¸  ä» ArXiv è·å–å¤±è´¥ï¼Œå›é€€åˆ° OpenAlex å…ƒæ•°æ®")
                            # ç»§ç»­ä½¿ç”¨ OpenAlex æ•°æ®
                    else:
                        logger.debug(f"    â„¹ï¸  [{title[:30]}...] æœªæ‰¾åˆ° arXiv ç‰ˆæœ¬ï¼Œä½¿ç”¨ OpenAlex å…ƒæ•°æ®")

                    # æ„å»ºè®ºæ–‡å…ƒæ•°æ®
                    metadata = PaperMetadata(
                        paper_id=doi,
                        title=title,
                        authors=authors,
                        abstract=abstract,
                        published_date=published_date,
                        url=landing_page_url,
                        source=journal_code,  # ä½¿ç”¨æœŸåˆŠä»£ç ä½œä¸º source
                        pdf_url=pdf_url,
                        doi=doi if not doi.startswith("openalex:") else None,
                        journal=journal_name,
                        arxiv_id=arxiv_id,
                        arxiv_url=arxiv_url
                    )
                    papers.append(metadata)
                    total_fetched += 1

                    if total_fetched >= self.max_results:
                        break

                # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ›´å¤šé¡µ
                page += 1
                if total_fetched >= self.max_results:
                    logger.debug(f"  å·²è¾¾åˆ°æœ€å¤§ç»“æœæ•° {self.max_results}ï¼Œåœæ­¢åˆ†é¡µ")
                    break

        except requests.exceptions.RequestException as e:
            logger.error(f"OpenAlex API è¯·æ±‚å¤±è´¥: {e}")
        except json.JSONDecodeError as e:
            logger.error(f"OpenAlex API å“åº”è§£æå¤±è´¥: {e}")
        except Exception as e:
            logger.error(f"OpenAlex æ•°æ®å¤„ç†å¤±è´¥: {e}")
            traceback.print_exc()

        logger.info(f"  å…±è·å– {len(papers)} ç¯‡è®ºæ–‡ï¼ˆåˆ† {page} é¡µï¼‰")
        return papers

    def _rebuild_abstract(self, inverted_index: Dict[str, List[int]]) -> str:
        """
        å°†å€’æ’ç´¢å¼•æ ¼å¼çš„æ‘˜è¦é‡å»ºä¸ºæ™®é€šæ–‡æœ¬ã€‚

        OpenAlex ä½¿ç”¨å€’æ’ç´¢å¼•å­˜å‚¨æ‘˜è¦ä»¥è§„é¿ç‰ˆæƒé—®é¢˜ã€‚
        æ ¼å¼: {"word": [position1, position2, ...], ...}

        å‚æ•°:
            inverted_index: å€’æ’ç´¢å¼•å­—å…¸

        è¿”å›:
            str: é‡å»ºçš„æ‘˜è¦æ–‡æœ¬
        """
        if not inverted_index:
            return ""

        try:
            # æ‰¾åˆ°æœ€å¤§ä½ç½®ç´¢å¼•
            max_position = 0
            for positions in inverted_index.values():
                if positions:
                    max_position = max(max_position, max(positions))

            # é˜²æ­¢å†…å­˜æº¢å‡ºï¼šé™åˆ¶æœ€å¤§positionå€¼
            MAX_ALLOWED_POSITION = 50000  # çº¦50KBçš„æ–‡æœ¬
            if max_position > MAX_ALLOWED_POSITION:
                logger.warning(f"æ‘˜è¦positionè¿‡å¤§ ({max_position})ï¼Œå¯èƒ½æ•°æ®æŸåï¼Œæˆªæ–­åˆ° {MAX_ALLOWED_POSITION}")
                max_position = MAX_ALLOWED_POSITION

            # åˆ›å»ºä½ç½®æ•°ç»„
            words_array = [""] * (max_position + 1)

            # å¡«å……å•è¯åˆ°å¯¹åº”ä½ç½®
            for word, positions in inverted_index.items():
                for pos in positions:
                    if 0 <= pos <= max_position:
                        words_array[pos] = word

            # åˆå¹¶ä¸ºæ–‡æœ¬
            abstract = " ".join(word for word in words_array if word)

            # åŸºæœ¬æ¸…ç†
            abstract = abstract.strip()

            return abstract

        except Exception as e:
            logger.warning(f"æ‘˜è¦é‡å»ºå¤±è´¥: {e}")
            return ""

    def _parse_date(self, date_str: str) -> datetime:
        """
        è§£æ OpenAlex è¿”å›çš„æ—¥æœŸã€‚

        OpenAlex æ—¥æœŸæ ¼å¼: "YYYY-MM-DD"

        å‚æ•°:
            date_str: æ—¥æœŸå­—ç¬¦ä¸²

        è¿”å›:
            datetime: è§£æåçš„æ—¥æœŸå¯¹è±¡
        """
        try:
            if date_str:
                return datetime.strptime(date_str, "%Y-%m-%d")
        except (ValueError, TypeError):
            pass

        return datetime.now()
