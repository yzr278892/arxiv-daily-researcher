"""
AI å…³é”®è¯æ ‡å‡†åŒ–æ¨¡å—

ä½¿ç”¨ LLM è¿›è¡Œå…³é”®è¯åˆå¹¶å’Œæ ‡å‡†åŒ–ã€‚
"""

import json
import logging
from typing import List, Dict, Optional
from pydantic import BaseModel
from openai import OpenAI

logger = logging.getLogger(__name__)


class NormalizationResult(BaseModel):
    """æ ‡å‡†åŒ–ç»“æœ"""
    canonical_form: str
    original_keywords: List[str]
    category: Optional[str] = None
    confidence: float = 1.0


class KeywordNormalizer:
    """
    AI å…³é”®è¯æ ‡å‡†åŒ–å™¨

    ä½¿ç”¨ cheap_llm è¿›è¡Œï¼š
    - åŒä¹‰è¯åˆå¹¶
    - ç¼©å†™å±•å¼€
    - æ‹¼å†™å˜ä½“ç»Ÿä¸€
    """

    def __init__(self):
        """åˆå§‹åŒ–ï¼Œä½¿ç”¨ settings ä¸­çš„ cheap_llm é…ç½®"""
        from config import settings
        self.client = OpenAI(
            api_key=settings.CHEAP_LLM.api_key,
            base_url=settings.CHEAP_LLM.base_url
        )
        self.model = settings.CHEAP_LLM.model_name

    def normalize_batch(
        self,
        keywords: List[str],
        existing_canonical: Optional[List[str]] = None,
        batch_size: int = 50
    ) -> List[NormalizationResult]:
        """
        æ‰¹é‡æ ‡å‡†åŒ–å…³é”®è¯

        Args:
            keywords: å¾…æ ‡å‡†åŒ–çš„å…³é”®è¯åˆ—è¡¨
            existing_canonical: å·²æœ‰çš„æ ‡å‡†å…³é”®è¯ï¼ˆä¼˜å…ˆæ˜ å°„ï¼‰
            batch_size: æ¯æ‰¹å¤„ç†æ•°é‡

        Returns:
            NormalizationResult åˆ—è¡¨
        """
        if not keywords:
            return []

        all_results = []

        # åˆ†æ‰¹å¤„ç†
        for i in range(0, len(keywords), batch_size):
            batch = keywords[i:i + batch_size]
            try:
                results = self._normalize_single_batch(batch, existing_canonical)
                all_results.extend(results)
            except Exception as e:
                logger.error(f"æ ‡å‡†åŒ–æ‰¹æ¬¡å¤±è´¥: {e}")
                # å¤±è´¥æ—¶ï¼Œæ¯ä¸ªå…³é”®è¯ä½œä¸ºç‹¬ç«‹çš„æ ‡å‡†å½¢å¼
                for kw in batch:
                    all_results.append(NormalizationResult(
                        canonical_form=kw,
                        original_keywords=[kw],
                        confidence=0.5
                    ))

        return all_results

    def _normalize_single_batch(
        self,
        keywords: List[str],
        existing_canonical: Optional[List[str]] = None
    ) -> List[NormalizationResult]:
        """å¤„ç†å•ä¸ªæ‰¹æ¬¡"""
        prompt = self._build_prompt(keywords, existing_canonical)

        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯å­¦æœ¯å…³é”®è¯æ ‡å‡†åŒ–ä¸“å®¶ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                response_format={"type": "json_object"}
            )

            content = response.choices[0].message.content

            # ğŸ†• ä¼˜åŒ–ï¼šæ£€æŸ¥contentæ˜¯å¦ä¸ºç©º
            if not content or not content.strip():
                logger.warning(f"LLMè¿”å›ç©ºå†…å®¹ï¼Œå…³é”®è¯: {keywords[:3]}...")
                raise ValueError("LLMè¿”å›ç©ºå†…å®¹")

            # ğŸ†• ä¼˜åŒ–ï¼šæ·»åŠ è¯¦ç»†æ—¥å¿—ç”¨äºè°ƒè¯•
            logger.debug(f"LLMè¿”å›å†…å®¹å‰100å­—ç¬¦: {content[:100]}")

            try:
                data = json.loads(content)
            except json.JSONDecodeError as e:
                # ğŸ†• ä¼˜åŒ–ï¼šè®°å½•å¯¼è‡´é”™è¯¯çš„åŸå§‹å†…å®¹
                logger.error(f"JSON è§£æå¤±è´¥: {e}")
                logger.error(f"åŸå§‹å†…å®¹: {content[:500]}")
                raise

            results = []
            normalizations = data.get("normalizations", [])

            # ğŸ†• ä¼˜åŒ–ï¼šæ£€æŸ¥è¿”å›æ•°æ®æ ¼å¼
            if not normalizations:
                logger.warning(f"LLMè¿”å›ç©ºçš„normalizationsåˆ—è¡¨")
                # è¿”å›ç©ºç»“æœï¼Œè®©ä¸Šå±‚å¤„ç†
                raise ValueError("è¿”å›çš„normalizationsä¸ºç©º")

            for norm in normalizations:
                results.append(NormalizationResult(
                    canonical_form=norm.get("canonical_form", "").lower(),
                    original_keywords=[kw.lower() for kw in norm.get("original_keywords", [])],
                    category=norm.get("category"),
                    confidence=norm.get("confidence", 0.9)
                ))

            return results

        except json.JSONDecodeError as e:
            logger.error(f"JSON è§£æå¤±è´¥: {e}")
            raise
        except Exception as e:
            logger.error(f"LLM è°ƒç”¨å¤±è´¥: {e}")
            raise

    def _build_prompt(
        self,
        keywords: List[str],
        existing_canonical: Optional[List[str]] = None
    ) -> str:
        """æ„å»ºæ ‡å‡†åŒ–æç¤º"""
        existing_str = ""
        if existing_canonical:
            existing_str = f"""
å·²çŸ¥çš„è§„èŒƒå…³é”®è¯åˆ—è¡¨ï¼ˆä¼˜å…ˆæ˜ å°„åˆ°è¿™äº›ï¼‰ï¼š
{json.dumps(existing_canonical[:50], ensure_ascii=False, indent=2)}
"""

        return f"""è¯·å¯¹ä»¥ä¸‹å­¦æœ¯å…³é”®è¯è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†ã€‚

ä»»åŠ¡ï¼š
1. è¯†åˆ«åŒä¹‰è¯ã€ç¼©å†™ã€æ‹¼å†™å˜ä½“ï¼Œå°†å®ƒä»¬åˆå¹¶ä¸ºè§„èŒƒå½¢å¼
2. é€‰æ‹©æœ€è§„èŒƒã€æœ€å¸¸ç”¨çš„å½¢å¼ä½œä¸º canonical_form
3. å¦‚æœå¯ä»¥å½’ç±»ï¼Œæä¾› categoryï¼ˆå¦‚ï¼šquantum, machine_learning, optimization, neural_network ç­‰ï¼‰
4. ç»™å‡ºå½’å¹¶çš„ç½®ä¿¡åº¦ï¼ˆ0.5-1.0ï¼‰
{existing_str}
å¾…å¤„ç†å…³é”®è¯ï¼š
{json.dumps(keywords, ensure_ascii=False, indent=2)}

è¾“å‡º JSON æ ¼å¼ï¼š
{{
  "normalizations": [
    {{
      "canonical_form": "quantum computing",
      "original_keywords": ["QC", "quantum computation", "quantum computing"],
      "category": "quantum",
      "confidence": 0.95
    }}
  ]
}}

è¦æ±‚ï¼š
- æ¯ä¸ªåŸå§‹å…³é”®è¯å¿…é¡»ä¸”åªèƒ½å‡ºç°åœ¨ä¸€ä¸ªç»„ä¸­
- ä¿æŒå­¦æœ¯æœ¯è¯­çš„å‡†ç¡®æ€§
- è‹±æ–‡å…³é”®è¯ç»Ÿä¸€ç”¨å°å†™ï¼ˆä¸“æœ‰åè¯é™¤å¤–ï¼‰
- å¦‚æœæŸä¸ªå…³é”®è¯æ— æ³•å½’ç±»ï¼Œå•ç‹¬ä½œä¸ºä¸€ç»„"""
