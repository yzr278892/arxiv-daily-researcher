"""
å¤šæ•°æ®æºè®ºæ–‡ç ”ç©¶ç³»ç»Ÿä¸»ç¨‹åº

æ”¯æŒä»å¤šä¸ªæ•°æ®æºæŠ“å–è®ºæ–‡ï¼š
- ArXivï¼šé¢„å°æœ¬è®ºæ–‡ï¼ˆæ”¯æŒPDFæ·±åº¦åˆ†æï¼‰
- å­¦æœ¯æœŸåˆŠï¼šPRLã€PRAç­‰ï¼ˆä»…è¯„åˆ†å’Œæ‘˜è¦ç¿»è¯‘ï¼‰
"""

from config import settings
from utils.logger import setup_logger
from agents import KeywordAgent, AnalysisAgent, Reporter
from agents.search_agent import SearchAgent
from agents.sources.base_source import PaperMetadata
from tqdm import tqdm
from typing import Dict, List, Any

# åˆå§‹åŒ–ç³»ç»Ÿæ—¥å¿—è®°å½•å™¨
logger = setup_logger("Main")


def main():
    """
    å¤šæ•°æ®æºç ”ç©¶ç³»ç»Ÿä¸»æµç¨‹ã€‚

    å·¥ä½œæµç¨‹:
    1. åŠ è½½é…ç½®
    2. å‡†å¤‡å…³é”®è¯ï¼ˆä¸»è¦å…³é”®è¯ + Referenceæå–çš„æ¬¡è¦å…³é”®è¯ï¼‰
    3. ä»å¤šä¸ªæ•°æ®æºæŠ“å–è®ºæ–‡
    4. å¯¹æ‰€æœ‰è®ºæ–‡è¿›è¡ŒåŠ æƒè¯„åˆ†
    5. å¯¹ArXivåŠæ ¼è®ºæ–‡è¿›è¡Œæ·±åº¦åˆ†æï¼ˆå…¶ä»–æ¥æºè·³è¿‡ï¼‰
    6. æŒ‰æ•°æ®æºåˆ†åˆ«ç”ŸæˆæŠ¥å‘Š
    """
    print("\n" + "=" * 80)
    print("ğŸš€ å¤šæ•°æ®æºç ”ç©¶ç³»ç»Ÿå¯åŠ¨")
    print("=" * 80 + "\n")

    logger.info("=" * 80)
    logger.info("å¯åŠ¨å¤šæ•°æ®æºç ”ç©¶ç³»ç»Ÿ")
    logger.info("=" * 80)

    # ==================== é˜¶æ®µ1: é…ç½®åŠ è½½ ====================
    logger.info(">>> é˜¶æ®µ1: åŠ è½½é…ç½®...")

    # æ‰“å°é…ç½®ä¿¡æ¯
    logger.info(f"å¯ç”¨çš„æ•°æ®æº: {settings.ENABLED_SOURCES}")
    if "arxiv" in settings.ENABLED_SOURCES:
        logger.info(f"ArXivç›®æ ‡é¢†åŸŸ: {settings.TARGET_DOMAINS}")
    if settings.TARGET_JOURNALS:
        logger.info(f"ç›®æ ‡æœŸåˆŠ: {settings.TARGET_JOURNALS}")
    logger.info(f"æœç´¢å¤©æ•°: {settings.SEARCH_DAYS}")
    logger.info(f"æœ€å¤§ç»“æœæ•°: {settings.MAX_RESULTS}")
    logger.info(f"å¯ç”¨Referenceæå–: {settings.ENABLE_REFERENCE_EXTRACTION}")

    # ==================== é˜¶æ®µ2: å…³é”®è¯å‡†å¤‡ ====================
    logger.info(">>> é˜¶æ®µ2: å‡†å¤‡å…³é”®è¯...")

    keyword_agent = KeywordAgent()

    # è·å–æ‰€æœ‰å…³é”®è¯ï¼ˆä¸»è¦ + Referenceæå–ï¼‰
    all_keywords = keyword_agent.get_all_keywords()

    if not all_keywords:
        logger.error("é”™è¯¯: æœªæ‰¾åˆ°ä»»ä½•å…³é”®è¯ã€‚è¯·åœ¨ search_config.json ä¸­é…ç½®ä¸»è¦å…³é”®è¯ã€‚")
        return

    logger.info("å…³é”®è¯å‡†å¤‡å®Œæˆ:")
    logger.info(f"  - ä¸»è¦å…³é”®è¯: {len(settings.PRIMARY_KEYWORDS)} ä¸ªï¼ˆæƒé‡ {settings.PRIMARY_KEYWORD_WEIGHT}ï¼‰")
    if settings.ENABLE_REFERENCE_EXTRACTION:
        ref_count = len(all_keywords) - len(settings.PRIMARY_KEYWORDS)
        logger.info(f"  - Referenceå…³é”®è¯: {ref_count} ä¸ªï¼ˆæƒé‡ 0.3-0.8ï¼‰")
    logger.info(f"  - å…³é”®è¯æ€»æ•°: {len(all_keywords)} ä¸ª")
    logger.info(f"  - æ€»æƒé‡: {sum(all_keywords.values()):.2f}")

    # è®¡ç®—åŠ¨æ€åŠæ ¼åˆ†
    total_weight = sum(all_keywords.values())
    passing_score = settings.calculate_passing_score(total_weight)
    logger.info(f"  - åŠ¨æ€åŠæ ¼åˆ†: {passing_score:.1f}")
    logger.info(f"  - åŠæ ¼åˆ†å…¬å¼: {settings.PASSING_SCORE_BASE} + {settings.PASSING_SCORE_WEIGHT_COEFFICIENT} Ã— {total_weight:.1f}")

    # ==================== é˜¶æ®µ3: æŠ“å–æ‰€æœ‰æœ€æ–°è®ºæ–‡ ====================
    logger.info(">>> é˜¶æ®µ3: ä»å¤šä¸ªæ•°æ®æºæŠ“å–è®ºæ–‡...")

    search_agent = SearchAgent(
        history_dir=settings.HISTORY_DIR,
        enabled_sources=settings.ENABLED_SOURCES,
        arxiv_domains=settings.TARGET_DOMAINS,
        journals=settings.TARGET_JOURNALS,
        max_results=settings.MAX_RESULTS,
        openalex_email=settings.OPENALEX_EMAIL,
        openalex_api_key=settings.OPENALEX_API_KEY,
        enable_semantic_scholar=settings.ENABLE_SEMANTIC_SCHOLAR_TLDR,
        semantic_scholar_api_key=settings.SEMANTIC_SCHOLAR_API_KEY
    )

    # ä»æ‰€æœ‰æ•°æ®æºæŠ“å–è®ºæ–‡
    papers_by_source: Dict[str, List[PaperMetadata]] = search_agent.fetch_all_papers(
        days=settings.SEARCH_DAYS
    )

    # è®¡ç®—æ€»æ•°
    total_papers_count = sum(len(papers) for papers in papers_by_source.values())

    if total_papers_count == 0:
        logger.info("æœªæ‰¾åˆ°æ–°è®ºæ–‡ã€‚")
        print("\næœªæ‰¾åˆ°æ–°è®ºæ–‡ï¼Œç¨‹åºé€€å‡ºã€‚")
        return

    logger.info(f"æˆåŠŸæŠ“å– {total_papers_count} ç¯‡æ–°è®ºæ–‡ï¼ˆæ¥è‡ª {len(papers_by_source)} ä¸ªæ•°æ®æºï¼‰")

    # ==================== é˜¶æ®µ4: å¯¹æ‰€æœ‰è®ºæ–‡è¯„åˆ† ====================
    logger.info(">>> é˜¶æ®µ4: å¯¹æ‰€æœ‰è®ºæ–‡è¿›è¡ŒåŠ æƒè¯„åˆ†...")

    analysis_agent = AnalysisAgent()
    scored_papers_by_source: Dict[str, List[Dict[str, Any]]] = {}

    for source, papers in papers_by_source.items():
        if not papers:
            continue

        logger.info(f"  è¯„åˆ†æ•°æ®æº [{source}]: {len(papers)} ç¯‡è®ºæ–‡")
        scored_papers = []

        # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦æ¡
        with tqdm(total=len(papers), desc=f"ğŸ“Š [{source}] è¯„åˆ†", unit="ç¯‡", ncols=100) as pbar:
            for idx, paper in enumerate(papers, 1):
                # æ›´æ–°è¿›åº¦æ¡æè¿°
                pbar.set_description(f"ğŸ“Š [{source}] [{idx}/{len(papers)}]")
                pbar.set_postfix_str(f"{paper.title[:35]}...")

                # ä½¿ç”¨åŠ æƒè¯„åˆ†ç³»ç»Ÿ
                score_response = analysis_agent.score_paper_with_keywords(
                    title=paper.title,
                    authors=paper.get_authors_string(),
                    abstract=paper.abstract,
                    keywords_dict=all_keywords
                )

                # ç¿»è¯‘æ‘˜è¦ï¼ˆä»…å½“æ‘˜è¦éç©ºæ—¶ï¼‰
                abstract_cn = analysis_agent.translate_abstract(paper.abstract) if paper.abstract and paper.abstract.strip() else ""

                # ä¿å­˜è®ºæ–‡ä¿¡æ¯å’Œè¯„åˆ†
                scored_papers.append({
                    'paper_metadata': paper,
                    'paper_id': paper.paper_id,
                    'title': paper.title,
                    'authors': paper.get_authors_string(),
                    'abstract': paper.abstract,
                    'abstract_cn': abstract_cn,
                    'url': paper.url,
                    'pdf_url': paper.pdf_url,
                    'published': paper.published_date.strftime('%Y-%m-%d') if paper.published_date else 'N/A',
                    'score_response': score_response
                })

                # è®°å½•æå–çš„å…³é”®è¯ç”¨äºè¶‹åŠ¿åˆ†æ
                if settings.KEYWORD_TRACKER_ENABLED and score_response.extracted_keywords:
                    try:
                        from agents.keyword_tracker import KeywordTracker
                        keyword_tracker = KeywordTracker()
                        keyword_tracker.record_keywords(
                            keywords=score_response.extracted_keywords,
                            paper_id=paper.paper_id,
                            source=source
                        )
                    except Exception as e:
                        logger.debug(f"å…³é”®è¯è®°å½•å¤±è´¥: {e}")

                # æ ‡è®°ä¸ºå·²å¤„ç†ï¼ˆé¿å…ä¸‹æ¬¡é‡å¤ï¼‰
                search_agent.mark_as_processed(paper.paper_id, source)

                # æ›´æ–°è¿›åº¦æ¡
                pbar.update(1)

        scored_papers_by_source[source] = scored_papers

        # ç»Ÿè®¡è¯¥æ•°æ®æºçš„åŠæ ¼è®ºæ–‡
        qualified_count = sum(1 for p in scored_papers if p['score_response'].is_qualified)
        logger.info(f"    [{source}] è¯„åˆ†å®Œæˆ: {qualified_count}/{len(papers)} ç¯‡åŠæ ¼")

    # ==================== é˜¶æ®µ5: æ·±åº¦åˆ†æåŠæ ¼è®ºæ–‡ ====================
    analyses_by_source: Dict[str, List[Dict[str, Any]]] = {}

    for source, scored_papers in scored_papers_by_source.items():
        qualified_papers = [p for p in scored_papers if p['score_response'].is_qualified]

        if not qualified_papers:
            logger.info(f">>> é˜¶æ®µ5: [{source}] æ²¡æœ‰åŠæ ¼è®ºæ–‡ï¼Œè·³è¿‡æ·±åº¦åˆ†æ")
            continue

        # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„PDFï¼ˆåŸå§‹PDFæˆ–arXiv PDFï¼‰
        papers_with_pdf = []
        for p in qualified_papers:
            paper_meta = p.get('paper_metadata')
            if paper_meta and paper_meta.has_pdf_access():
                papers_with_pdf.append(p)

        if not papers_with_pdf:
            logger.info(f">>> é˜¶æ®µ5: [{source}] {len(qualified_papers)} ç¯‡åŠæ ¼è®ºæ–‡å‡æ— PDFå¯ç”¨ï¼Œè·³è¿‡æ·±åº¦åˆ†æ")
            continue

        logger.info(f">>> é˜¶æ®µ5: [{source}] æ·±åº¦åˆ†æ {len(papers_with_pdf)}/{len(qualified_papers)} ç¯‡æœ‰PDFçš„åŠæ ¼è®ºæ–‡...")

        qualified_papers_with_analysis = []

        # ä½¿ç”¨tqdmæ˜¾ç¤ºæ·±åº¦åˆ†æè¿›åº¦
        with tqdm(total=len(papers_with_pdf), desc=f"ğŸ”¬ [{source}] æ·±åº¦åˆ†æ", unit="ç¯‡", ncols=100) as pbar:
            for idx, paper_info in enumerate(papers_with_pdf, 1):
                # æ›´æ–°è¿›åº¦æ¡
                pbar.set_description(f"ğŸ”¬ [{source}] [{idx}/{len(papers_with_pdf)}]")
                pbar.set_postfix_str(f"{paper_info['title'][:35]}...")

                # è·å–æœ€ä½³çš„PDF URL
                paper_meta = paper_info.get('paper_metadata')
                pdf_url = paper_meta.get_best_pdf_url() if paper_meta else paper_info.get('pdf_url')

                # ä¸‹è½½PDFå¹¶æ·±åº¦åˆ†æ
                analysis = analysis_agent.deep_analyze(
                    title=paper_info['title'],
                    pdf_url=pdf_url,
                    abstract=paper_info['abstract'],
                    fallback_to_abstract=True
                )

                if analysis:
                    qualified_papers_with_analysis.append({
                        'paper_id': paper_info['paper_id'],
                        'analysis': analysis
                    })
                    # æ˜¾ç¤ºarXivæ¥æºä¿¡æ¯
                    if paper_meta and paper_meta.arxiv_id:
                        pbar.write(f"  âœ“ å®Œæˆ (via arXiv {paper_meta.arxiv_id}): {paper_info['title'][:50]}...")
                    else:
                        pbar.write(f"  âœ“ å®Œæˆ: {paper_info['title'][:55]}...")
                else:
                    pbar.write(f"  âœ— å¤±è´¥: {paper_info['title'][:55]}...")

                pbar.update(1)

        analyses_by_source[source] = qualified_papers_with_analysis
        logger.info(f"    [{source}] æ·±åº¦åˆ†æå®Œæˆ: {len(qualified_papers_with_analysis)}/{len(papers_with_pdf)} ç¯‡æˆåŠŸ")

    # ==================== é˜¶æ®µ6: ç”Ÿæˆåˆ†æ•°æ®æºæŠ¥å‘Š ====================
    logger.info(">>> é˜¶æ®µ6: ç”Ÿæˆåˆ†æ•°æ®æºç ”ç©¶æŠ¥å‘Š...")

    reporter = Reporter()
    report_paths = reporter.generate_reports_by_source(
        scored_papers_by_source=scored_papers_by_source,
        keywords_dict=all_keywords,
        analyses_by_source=analyses_by_source
    )

    # ==================== é˜¶æ®µ7: å…³é”®è¯è¶‹åŠ¿å¤„ç† ====================
    if settings.KEYWORD_TRACKER_ENABLED and settings.KEYWORD_NORMALIZATION_ENABLED:
        logger.info(">>> é˜¶æ®µ7: è¿è¡Œæ¯æ—¥å…³é”®è¯æ ‡å‡†åŒ–...")
        try:
            from agents.keyword_tracker import KeywordTracker
            tracker = KeywordTracker()
            stats = tracker.run_daily_normalization()
            logger.info(f"  æ ‡å‡†åŒ–å®Œæˆ: å¤„ç† {stats['processed']} ä¸ª, æ–°å¢è§„èŒƒè¯ {stats['new_canonical']}, åˆå¹¶ {stats['merged']}")

            # æ ¹æ®é…ç½®çš„é¢‘ç‡å†³å®šæ˜¯å¦ç”Ÿæˆè¶‹åŠ¿æŠ¥å‘Š
            if settings.KEYWORD_REPORT_ENABLED:
                from datetime import date
                today = date.today()
                should_generate_report = False

                if settings.KEYWORD_REPORT_FREQUENCY == "always":
                    should_generate_report = True
                elif settings.KEYWORD_REPORT_FREQUENCY == "daily":
                    should_generate_report = True
                elif settings.KEYWORD_REPORT_FREQUENCY == "weekly":
                    # æ¯å‘¨ä¸€ç”Ÿæˆ
                    should_generate_report = (today.weekday() == 0)
                elif settings.KEYWORD_REPORT_FREQUENCY == "monthly":
                    # æ¯æœˆ1å·ç”Ÿæˆ
                    should_generate_report = (today.day == 1)

                if should_generate_report:
                    logger.info("  ç”Ÿæˆå…³é”®è¯è¶‹åŠ¿æŠ¥å‘Š...")
                    trend_report_path = settings.REPORTS_DIR / f"keyword_trends_{today.isoformat()}.md"
                    bar_chart = tracker.generate_bar_chart()
                    trend_chart = tracker.generate_trend_chart()
                    top_keywords = tracker.get_top_keywords()

                    with open(trend_report_path, 'w', encoding='utf-8') as f:
                        f.write(f"# å…³é”®è¯è¶‹åŠ¿åˆ†ææŠ¥å‘Š\n\n")
                        f.write(f"ç”Ÿæˆæ—¥æœŸ: {today}\n\n")
                        f.write("## çƒ­é—¨å…³é”®è¯æ’å\n\n")
                        if bar_chart:
                            f.write(bar_chart + "\n\n")
                        f.write("## å…³é”®è¯è¶‹åŠ¿å˜åŒ–\n\n")
                        if trend_chart:
                            f.write(trend_chart + "\n\n")
                        f.write("## ç»Ÿè®¡è¡¨æ ¼\n\n")
                        f.write("| Rank | Keyword | Count | Category |\n")
                        f.write("|------|---------|-------|----------|\n")
                        for i, kw in enumerate(top_keywords, 1):
                            f.write(f"| {i} | {kw['keyword']} | {kw['count']} | {kw.get('category') or '-'} |\n")

                    logger.info(f"  è¶‹åŠ¿æŠ¥å‘Šå·²ä¿å­˜: {trend_report_path}")
                else:
                    logger.info(f"  è·³è¿‡è¶‹åŠ¿æŠ¥å‘Šç”Ÿæˆ (é¢‘ç‡è®¾ç½®: {settings.KEYWORD_REPORT_FREQUENCY})")

        except Exception as e:
            logger.warning(f"å…³é”®è¯æ ‡å‡†åŒ–å¤±è´¥: {e}")

    # ==================== å®Œæˆ ====================
    logger.info("=" * 80)
    logger.info("âœ… ä»»åŠ¡å®Œæˆï¼")

    # ç»Ÿè®¡å„æ•°æ®æº
    total_qualified = 0
    total_analyzed = 0

    for source, scored_papers in scored_papers_by_source.items():
        source_qualified = sum(1 for p in scored_papers if p['score_response'].is_qualified)
        source_analyzed = len(analyses_by_source.get(source, []))
        total_qualified += source_qualified
        total_analyzed += source_analyzed
        logger.info(f"  [{source}] æŠ“å–: {len(scored_papers)} | åŠæ ¼: {source_qualified} | æ·±åº¦åˆ†æ: {source_analyzed}")

    logger.info(f"  - æ€»è®¡: æŠ“å– {total_papers_count} | åŠæ ¼ {total_qualified} | æ·±åº¦åˆ†æ {total_analyzed}")
    logger.info(f"  - æŠ¥å‘Šä½ç½®: {settings.REPORTS_DIR}")
    logger.info("=" * 80)

    # æ‰“å°æ§åˆ¶å°æ‘˜è¦
    print("\n" + "=" * 80)
    print("ğŸ‰ æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆï¼")
    print("=" * 80)
    print("ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")

    for source, scored_papers in scored_papers_by_source.items():
        source_qualified = sum(1 for p in scored_papers if p['score_response'].is_qualified)
        source_analyzed = len(analyses_by_source.get(source, []))
        pct = (source_qualified / len(scored_papers) * 100) if scored_papers else 0
        print(f"   [{source.upper()}]")
        print(f"     â€¢ æŠ“å–: {len(scored_papers)} ç¯‡")
        print(f"     â€¢ åŠæ ¼: {source_qualified} ç¯‡ ({pct:.1f}%)")
        if search_agent.can_download_pdf(source):
            print(f"     â€¢ æ·±åº¦åˆ†æ: {source_analyzed} ç¯‡")

    print(f"\nğŸ“ æŠ¥å‘Šä½ç½®:")
    for source, path in report_paths.items():
        print(f"   â€¢ [{source}] {path}")

    print("=" * 80 + "\n")


if __name__ == "__main__":
    settings.ensure_directories()
    main()
